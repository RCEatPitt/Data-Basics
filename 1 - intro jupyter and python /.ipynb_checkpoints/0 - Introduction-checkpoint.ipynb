{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we care about Python?\n",
    "\n",
    "* We can use Python to automate boring a repetitive tasks\n",
    "* We can use Python to clean, transform, and manipulate data (not to mention perform analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Data: The Federalist papers\n",
    "\n",
    "\n",
    "* Who wrote the Federalist Papers? Alexander Hamilton, James Madison, or John Jay?  \n",
    "* For more than 150 years, historians argued over the authorship of the 12 essays in _The Federalist Papers_. \n",
    "* It wasn't until 1963 that the mystery was solved by Frederick Mosteller of Harvard University and David Wallace of the University of Chicago. They used statistics and computation to analyze [the style of the text](https://en.wikipedia.org/wiki/Stylometry) and determine the authors of each paper.\n",
    "\n",
    "Full text of _The Federalist Papers_ is available at http://www.gutenberg.org/ebooks/1404\n",
    "\n",
    "### Looking at the data\n",
    "\n",
    "<- Open the file called `federalist_papers.txt` in JupyterLab to see the whole file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data into Python\n",
    "\n",
    "* We can use Python to manipulate the file, but first we need to load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to our data file (source file)\n",
    "source_file_name = \"federalist_papers.txt\"\n",
    "\n",
    "# open the text file and read it into memory\n",
    "with open(source_file_name) as f:\n",
    "    fed_papers_text = f.read()\n",
    "\n",
    "# Display the first 1000 characters\n",
    "print(fed_papers_text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that we have the text file loaded we can begin to manipulate it.\n",
    "* The main thing we want to do is calculate *word frequencies*.\n",
    "    * Let's count the frequency of the words \"while\" and \"whilst\"\n",
    "* So first we need to divide it up into separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the text into separate words based on spaces\n",
    "word_list = fed_papers_text.split(\" \")\n",
    "\n",
    "#display the first 100 words\n",
    "print(word_list[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Will this work?  Are words always separated by spaces?\n",
    "* While there are specialized Python libraries for dealing with text parsing (for example, [spaCy](https://spacy.io/) and the [natural language toolkit](https://www.nltk.org/), we are going to keep it simple.\n",
    "* For now we'll clean our text data in a quick and dirty way by removing the main punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# make a list of punctuation marks\n",
    "punctuation_marks = ['!','.', ',', ':', ';', '?', '-', '\\n','\\ufeff']\n",
    "# Remove them all from the text and replace with spaces\n",
    "for pm in punctuation_marks:\n",
    "    fed_papers_text = fed_papers_text.replace(pm, '')\n",
    "                     \n",
    "# Display the first 1000 characters\n",
    "print(fed_papers_text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice we removed a bunch of the formatting too.\n",
    "* We still have a little bit of text processing to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It would be a good idea to convert everything to lower case before we do anything else\n",
    "fed_papers_text = fed_papers_text.lower()\n",
    "\n",
    "# Now let's build a list of words\n",
    "word_list = fed_papers_text.split(\" \")\n",
    "\n",
    "#display the first 100 words\n",
    "print(word_list[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now that we have the text a bit more *normalized* we can start counting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the frequency for \"while\" and \"whilst\"\n",
    "\n",
    "# create some variables to track the count\n",
    "freq_while = 0\n",
    "freq_whilst = 0\n",
    "\n",
    "# loop over every word in the word list\n",
    "for word in word_list:\n",
    "    \n",
    "    # is the word while?\n",
    "    if word == \"while\":\n",
    "        # add one to our while variable\n",
    "        freq_while = freq_while + 1 \n",
    "    # is the word whilst?\n",
    "    elif word == \"whilst\":\n",
    "        # add one to our while variable\n",
    "        freq_whilst = freq_whilst + 1\n",
    "    # the word is neither while or whilst\n",
    "    else:\n",
    "        # just continue looping\n",
    "        continue\n",
    "        \n",
    "print(\"The frequency of 'while' is:\", freq_while)\n",
    "print(\"The frequency of 'whilst' is:\", freq_whilst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Look at that! We figured out who wrote the federalist papers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
