{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databases and SQL\n",
    "---\n",
    "In the late 1920s and early 1930s, William Dyer, Frank Pabodie, and Valentina Roerich led expeditions to the Pole of Inaccessibility in the South Pacific, and then onward to Antarctica. Two years ago, their expeditions were found in a storage locker at Miskatonic University. We have scanned and OCR the data they contain, and we now want to store that information in a way that will make search and analysis easy.\n",
    "\n",
    "Three common options for storage are text files, spreadsheets, and databases. Text files are easiest to create, and work well with version control, but then we would have to build search and analysis tools ourselves. Spreadsheets are good for doing simple analyses, but they don’t handle large or complex data sets well. Databases, however, include powerful tools for search and analysis, and can handle large, complex data sets. These lessons will show how to use a database to explore the expeditions’ data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python and SQL\n",
    "---\n",
    "The sqlite3 that we will be using throughout this tutorial is part of the Python Standard Library and is a nice and easy interface to SQLite databases: There are no server processes involved, no configurations required, and no other obstacles we have to worry about.\n",
    "\n",
    "First we'll introduce you to the sqlite3 module. In general, the only thing that needs to be done before we can perform any operation on a SQLite database via Python’s sqlite3 module, is to open a connection to an SQLite database file where the database file (sqlite_file) can reside anywhere on our disk, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "sqlite_file = 'survey.db'\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually execute a query, we can use the execute() method with our cursor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('SELECT * from Person LIMIT 5;')\n",
    "\n",
    "# Next, we store all of the results in a variable\n",
    "all_rows = c.fetchall()\n",
    "all_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this doesn't give us a nicely formatted result. All of the queried data is there, but it's not exactly what we would see if executing the query with the command line. Fortunately, pandas has us covered. Using the read_sql() method, we can store queried results in a dataframe! We still need to open a connection to the database and pass it in as an argument, but pandas takes care of the rest:\n",
    "\n",
    "To round up this section about connecting to a SQLite database file, there are two more operations that are worth mentioning. If we are finished with our operations on the database file, we have to close the connection via the .close() method. If we performed any operation on the database other than sending queries, we need to commit those changes via the .commit() method before we close the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql('SELECT * from Person LIMIT 5;', conn)\n",
    "df\n",
    "\n",
    "# you may also want to store your query string in a variable for easier reading\n",
    "query = '''\n",
    "SELECT * from Person\n",
    "LIMIT 5;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this data in the same way as we have all of our previous dataframes! The rest of this lecture will use pandas.\n",
    "\n",
    "To round up this section about connecting to a SQLite database file, there are two more operations that are worth mentioning. If we are finished with our operations on the database file, we have to close the connection via the .close() method. If we performed any operation on the database other than sending queries, we need to commit those changes via the .commit() method before we close the connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data\n",
    "---\n",
    "A relational database is a way to store and manipulate information. Databases are arranged as tables. Each table has columns (also known as fields) that describe the data, and rows (also known as records) which contain the data.\n",
    "\n",
    "When we are using a spreadsheet, we put formulas into cells to calculate new values based on old ones. When we are using a database, we send commands (usually called queries) to a database manager: a program that manipulates the database for us. The database manager does whatever lookups and calculations the query specifies, returning the results in a tabular form that we can then use as a starting point for further queries.\n",
    "\n",
    "Queries are written in a language called SQL, which stands for “Structured Query Language”. SQL provides hundreds of different ways to analyze and recombine data. We will only look at a handful of queries, but that handful accounts for most of what scientists do.\n",
    "\n",
    "For now, let’s write an SQL query that displays scientists’ names. We do this using the SQL command SELECT, giving it the names of the columns we want and the table we want them from. Our query and its output look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reopen our closed connection\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "query = '''\n",
    "SELECT family, personal \n",
    "FROM Person;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semicolon at the end of the query tells the database manager that the query is complete and ready to run. We have written our commands in upper case and the names for the table and columns in lower case, but we don’t have to: as the example below shows, SQL is case insensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SeLeCt FaMiLy, PeRsOnAl \n",
    "FrOm PeRsOn;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use SQL’s case insensitivity to your advantage. For instance, some people choose to write SQL keywords (such as SELECT and FROM) in capital letters and field and table names in lower case. This can make it easier to locate parts of an SQL statement. For instance, you can scan the statement, quickly locate the prominent FROM keyword and know the table name follows. Whatever casing convention you choose, please be consistent: complex queries are hard enough to read without the extra cognitive load of random capitalization. One convention is to use UPPER CASE for SQL statements, to distinguish them from tables and column names. This is the convention that we will use for this lesson.\n",
    "\n",
    "Now, going back to our query, it’s important to understand that the rows and columns in a database table aren’t actually stored in any particular order. They will always be displayed in some order, but we can control that in various ways. For example, we could swap the columns in the output by writing our query as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT personal, family \n",
    "FROM Person;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or even repeat columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT id, id, id \n",
    "FROM Person;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a shortcut, we can select all of the columns in a table using *:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Person;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To practice, write a query that select only the name column from the Site table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points:\n",
    "- A relational database stores information in tables, each of which has a fixed set of columns and a variable number of records.\n",
    "- A database manager is a program that manipulates information stored in a database.\n",
    "- We write queries in a specialized language called SQL to extract information from databases.\n",
    "- Use SELECT… FROM… to get values from a database table.\n",
    "- SQL is case-insensitive (but data is case-sensitive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting and Removing Duplicates\n",
    "---\n",
    "In beginning our examination of the Antarctic data, we want to know:\n",
    "\n",
    "- what kind of quantity measurements were taken at each site;\n",
    "- which scientists took measurements on the expedition;\n",
    "- the sites where each scientist took measurements\n",
    "\n",
    "To determine which measurements were taken at each site, we can examine the Survey table. Data is often redundant, so queries often return redundant information. For example, if we select the quantities that have been measured from the Survey table, we get this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT quant \n",
    "FROM Survey;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result makes it difficult to see all of the different types of quant in the Survey table. We can eliminate the redundant output to make the result more readable by adding the DISTINCT keyword to our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT quant \n",
    "FROM Survey;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to determine which visit (stored in the taken column) have which quant measurement, we can use the DISTINCT keyword on multiple columns. If we select more than one column, distinct sets of values are returned (in this case pairs, because we are selecting two columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT taken, quant \n",
    "FROM Survey;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in both cases that duplicates are removed even if the rows they come from didn’t appear to be adjacent in the database table.\n",
    "\n",
    "Our next task is to identify the scientists on the expedition by looking at the Person table. As we mentioned earlier, database records are not stored in any particular order. This means that query results aren’t necessarily sorted, and even if they are, we often want to sort them in a different way, e.g., by their identifier instead of by their personal name. We can do this in SQL by adding an ORDER BY clause to our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Person \n",
    "ORDER BY id\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, when we use ORDER BY results are sorted in ascending order of the column we specify (i.e., from least to greatest).\n",
    "\n",
    "Note: While it may look that the records are consistent every time we ask for them in this lesson, that is because no one has changed or modified any of the data so far. Remember to use ORDER BY if you want the rows returned to have any sort of consistent or predictable order.\n",
    "\n",
    "We can sort in the opposite order using DESC (for “descending”):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM person \n",
    "ORDER BY id DESC;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to make it clear that we’re sorting in ascending order, we can use ASC instead of DESC.\n",
    "\n",
    "In order to look at which scientist measured quantities during each visit, we can look again at the Survey table. We can also sort on several fields at once. For example, this query sorts results first in ascending order by taken, and then in descending order by person within each group of equal taken values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT taken, person, quant \n",
    "FROM Survey \n",
    "ORDER BY taken ASC, person DESC;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query gives us a good idea of which scientist was involved in which visit, and what measurements they performed during the visit.\n",
    "\n",
    "Looking at the table, it seems like some scientists specialized in certain kinds of measurements. We can examine which scientists performed which measurements by selecting the appropriate columns and removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT quant, person \n",
    "FROM Survey \n",
    "ORDER BY quant ASC;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To practice, write a query that selects distinct dates from the Visited table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a query that displays the full names of the scientist in the Person table, ordered by family name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points:\n",
    "- The records in a database table are not intrinsically ordered: if we want to display them in some order, we must specify that explicitly with ORDER BY.\n",
    "- The values in a database are not guaranteed to be unique: if we want to eliminate duplicates, we must specify that explicitly as well using DISTINCT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "---\n",
    "One of the most powerful features of a database is the ability to filter data, i.e., to select only those records that match certain criteria. For example, suppose we want to see when a particular site was visited. We can select these records from the Visited table by using a WHERE clause in our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Visited \n",
    "WHERE site = \"DR-1\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database manager executes this query in two stages. First, it checks at each row in the Visited table to see which ones satisfy the WHERE. It then uses the column names following the SELECT keyword to determine which columns to display.\n",
    "\n",
    "This processing order means that we can filter records using WHERE based on values in columns that aren’t then displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT id \n",
    "FROM Visited \n",
    "WHERE site = \"DR-1\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Processing Order](https://swcarpentry.github.io/sql-novice-survey/fig/sql-filter.svg)\n",
    "\n",
    "We can use many other Boolean operators to filter our data. For example, we can ask for all information from the DR-1 site collected before 1930:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Visited \n",
    "WHERE site = \"DR-1\" AND dated < \"1930-01-01\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on date types: Most database managers have a special data type for dates. In fact, many have two: one for dates, such as “May 31, 1971”, and one for durations, such as “31 days”. SQLite doesn’t: instead, it stores dates as either text (in the ISO-8601 standard format “YYYY-MM-DD HH:MM:SS.SSSS”), real numbers (Julian days, the number of days since November 24, 4714 BCE), or integers (Unix time, the number of seconds since midnight, January 1, 1970). If this sounds complicated, it is, but not nearly as complicated as figuring out historical dates in Sweden.\n",
    "\n",
    "If we want to find out what measurements were taken by either Lake or Roerich, we can combine the tests on their names using OR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Survey \n",
    "WHERE person = \"lake\" OR person = \"roe\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use IN to see if a value is in a specific set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Survey \n",
    "WHERE person IN (\"lake\", \"roe\");\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine AND with OR, but we need to be careful about which operator is executed first. If we don’t use parentheses, we get this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Survey \n",
    "WHERE quant = \"sal\" AND person = \"lake\" OR person = \"roe\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is salinity measurements by Lake, and any measurement by Roerich. We probably want this instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Survey \n",
    "WHERE quant = \"sal\" AND (person = \"lake\" OR person = \"roe\");\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter by partial matches. For example, if we want to know something just about the site names beginning with “DR” we can use the LIKE keyword. The percent symbol acts as a wildcard, matching any characters in that place. It can be used at the beginning, middle, or end of the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Visited \n",
    "WHERE site LIKE \"DR%\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use DISTINCT with WHERE to give a second level of filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT person, quant \n",
    "FROM Survey \n",
    "WHERE person = \"lake\" OR person = \"roe\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: DISTINCT is applied to the values displayed in the chosen columns, not to the entire rows as they are being processed.\n",
    "\n",
    "What we have just done is how most people “grow” their SQL queries. We started with something simple that did part of what we wanted, then added more clauses one by one, testing their effects as we went. This is a good strategy — in fact, for complex queries it’s often the only strategy — but it depends on quick turnaround, and on us recognizing the right answer when we get it.\n",
    "\n",
    "The best way to achieve quick turnaround is often to put a subset of data in a temporary database and run our queries against that, or to fill a small database with synthesized records. For example, instead of trying our queries against an actual database of 20 million Australians, we could run it against a sample of ten thousand, or write a small program to generate ten thousand random (but plausible) records and use that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix this query: Suppose we want to select all sites that lie more than 42 degrees from the poles. Explain why this is wrong, and rewrite the query so that it is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Site \n",
    "WHERE (lat > -48) OR (lat < 48);\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding outliers: Normalized salinity readings are supposed to be between 0.0 and 1.0. Write a query that selects all records from Survey with salinity values outside this range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points:\n",
    "- Use WHERE to specify conditions that records must meet in order to be included in a query’s results.\n",
    "- Use AND, OR, and NOT to combine tests.\n",
    "- Filtering is done on whole records, so conditions can use fields that are not actually displayed.\n",
    "- Write queries incrementally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating New Values\n",
    "---\n",
    "After carefully re-reading the expedition logs, we realize that the radiation measurements they report may need to be corrected upward by 5%. Rather than modifying the stored data, we can do this calculation on the fly as part of our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT 1.05 * reading \n",
    "FROM Survey \n",
    "WHERE quant = \"rad\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the query, the expression 1.05 * reading is evaluated for each row. Expressions can use any of the fields, all of usual arithmetic operators, and a variety of common functions. (Exactly which ones depends on which database manager is being used.) For example, we can convert temperature readings from Fahrenheit to Celsius and round to two decimal places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT taken, round(5 * (reading - 32) / 9, 2) \n",
    "FROM Survey \n",
    "WHERE quant = \"temp\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from this example, though, the string describing our new field (generated from the equation) can become quite unwieldy. SQL allows us to rename our fields, any field for that matter, whether it was calculated or one of the existing fields in our database, for succinctness and clarity. For example, we could write the previous query as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT taken, round(5 * (reading - 32) / 9, 2) as Celsius \n",
    "FROM Survey \n",
    "WHERE quant = \"temp\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine values from different fields, for example by using the string concatenation operator ||:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT personal || \" \" || family \n",
    "FROM Person;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing salinity reading: After further reading, we realize that Valentina Roerich was reporting salinity as percentages. Write a query that returns all of her salinity measurements from the Survey table with the values divided by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UNION operator combines the results of two queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Person \n",
    "WHERE id = \"dyer\" UNION SELECT * \n",
    "                        FROM Person \n",
    "                        WHERE id = \"roe\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use UNION to create a consolidated list of salinity measurements in which Valentina Roerich’s, and only Valentina’s, have been corrected as described in the previous challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting major site identifiers: The site identifiers in the Visited table have two parts separated by a ‘-‘:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT DISTINCT site \n",
    "FROM Visited;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some major site identifiers (i.e. the letter codes) are two letters long and some are three. The “in string” function instr(X, Y) returns the 1-based index of the first occurrence of string Y in string X, or 0 if Y does not exist in X. The substring function substr(X, I, [L]) returns the substring of X starting at index I, with an optional length L. Use these two functions to produce a list of unique major site identifiers. (For this data, the list should contain only “DR” and “MSK”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points:\n",
    "- Queries can do the usual arithmetic operations on values.\n",
    "- Use UNION to combine the results of two or more queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "---\n",
    "Real-world data is never complete — there are always holes. Databases represent these holes using a special value called null. null is not zero, False, or the empty string; it is a one-of-a-kind value that means “nothing here”. Dealing with null requires a few special tricks and some careful thinking.\n",
    "\n",
    "To start, let’s have a look at the Visited table. There are eight records, but #752 doesn’t have a date — or rather, its date is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Visited;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null doesn’t behave like other values. If we select the records that come before 1930:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Visited \n",
    "WHERE dated < \"1930-01-01\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get two results, and if we select the ones that come during or after 1930:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Visited \n",
    "WHERE dated >= \"1930-01-01\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get five, but record #752 isn’t in either set of results. The reason is that null<'1930-01-01' is neither true nor false: null means, “We don’t know,” and if we don’t know the value on the left side of a comparison, we don’t know whether the comparison is true or false. Since databases represent “don’t know” as null, the value of null<'1930-01-01' is actually null. null>='1930-01-01' is also null because we can’t answer to that question either. And since the only records kept by a WHERE are those for which the test is true, record #752 isn’t included in either set of results.\n",
    "\n",
    "Comparisons aren’t the only operations that behave this way with nulls. 1+null is null, 5*null is null, log(null) is null, and so on. In particular, comparing things to null with = and != produces null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Visited \n",
    "WHERE dated = NULL;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "produces no output, and neither does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Visited \n",
    "WHERE dated != NULL;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether a value is null or not, we must use a special test IS NULL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Visited \n",
    "WHERE dated IS NULL;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or its inverse IS NOT NULL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Visited \n",
    "WHERE dated IS NOT NULL;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values can cause headaches wherever they appear. For example, suppose we want to find all the salinity measurements that weren’t taken by Lake. It’s natural to write the query like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Survey \n",
    "WHERE quant = \"sal\" AND person != \"lake\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but this query filters omits the records where we don’t know who took the measurement. Once again, the reason is that when person is null, the != comparison produces null, so the record isn’t kept in our results. If we want to keep these records we need to add an explicit check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * \n",
    "FROM Survey \n",
    "WHERE quant = \"sal\" AND (person != \"lake\" OR person IS NULL);\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have to decide whether this is the right thing to do or not. If we want to be absolutely sure that we aren’t including any measurements by Lake in our results, we need to exclude all the records for which we don’t know who did the work.\n",
    "\n",
    "In contrast to arithmetic or Boolean operators, aggregation functions that combine multiple values, such as min, max or avg, ignore null values. In the majority of cases, this is a desirable output: for example, unknown values are thus not affecting our data when we are averaging it. Aggregation functions will be addressed in more detail in the next section.\n",
    "\n",
    "Write a query that sorts the records in Visited by date, omitting entries for which the date is not known (i.e., is null):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you expect the query:\n",
    "\n",
    "SELECT * FROM Visited WHERE dated IN ('1927-02-08', NULL);\n",
    "\n",
    "to produce? What does it actually produce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentinels: Some database designers prefer to use a sentinel value to mark missing data rather than null. For example, they will use the date “0000-00-00” to mark a missing date, or -1.0 to mark a missing salinity or radiation reading (since actual readings cannot be negative). What does this simplify? What burdens or risks does it introduce?\n",
    "\n",
    "### Key Points:\n",
    "- Databases use a special value called NULL to represent missing information.\n",
    "- Almost all operations on NULL produce NULL.\n",
    "- Queries can test for NULLs using IS NULL and IS NOT NULL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "---\n",
    "We now want to calculate ranges and averages for our data. We know how to select all of the dates from the Visited table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT dated \n",
    "FROM Visited;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but to combine them, we must use an aggregation function such as min or max. Each of these functions takes a set of records as input, and produces a single record as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT min(dated) \n",
    "FROM Visited;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Processing Order](https://swcarpentry.github.io/sql-novice-survey/fig/sql-aggregation.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT max(dated) \n",
    "FROM Visited;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min and max are just two of the aggregation functions built into SQL. Three others are avg, count, and sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT avg(reading) \n",
    "FROM Survey \n",
    "WHERE quant = \"sal\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT count(reading) \n",
    "FROM Survey \n",
    "WHERE quant = \"sal\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT sum(reading) \n",
    "FROM Survey \n",
    "WHERE quant = \"sal\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used count(reading) here, but we could just as easily have counted quant or any other field in the table, or even used count(*), since the function doesn’t care about the values themselves, just how many values there are.\n",
    "\n",
    "SQL lets us do several aggregations at once. We can, for example, find the range of sensible salinity measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT min(reading), max(reading) \n",
    "FROM Survey \n",
    "WHERE quant = \"sal\" AND reading <= 1.0;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine aggregated results with raw results, although the output might surprise you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT person, count(*) \n",
    "FROM Survey \n",
    "WHERE quant = \"sal\" AND reading <= 1.0;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does Lake’s name appear rather than Roerich’s or Dyer’s? The answer is that when it has to aggregate a field, but isn’t told how to, the database manager chooses an actual value from the input set. It might use the first one processed, the last one, or something else entirely.\n",
    "\n",
    "Another important fact is that when there are no values to aggregate — for example, where there are no rows satisfying the WHERE clause — aggregation’s result is “don’t know” rather than zero or some other arbitrary value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT person, max(reading), sum(reading) \n",
    "FROM Survey \n",
    "WHERE quant = \"missing\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final important feature of aggregation functions is that they are inconsistent with the rest of SQL in a very useful way. If we add two values, and one of them is null, the result is null. By extension, if we use sum to add all the values in a set, and any of those values are null, the result should also be null. It’s much more useful, though, for aggregation functions to ignore null values and only combine those that are non-null. This behavior lets us write our queries as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT min(dated) \n",
    "FROM Visited;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of always having to filter explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT min(dated) \n",
    "FROM Visited \n",
    "WHERE dated IS NOT NULL;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating all records at once doesn’t always make sense. For example, suppose we suspect that there is a systematic bias in our data, and that some scientists’ radiation readings are higher than others. We know that this doesn’t work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT person, count(reading), round(avg(reading), 2) \n",
    "FROM  Survey \n",
    "WHERE quant = \"rad\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the database manager selects a single arbitrary scientist’s name rather than aggregating separately for each scientist. Since there are only five scientists, we could write five queries of the form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT person, count(reading), round(avg(reading), 2) \n",
    "FROM  Survey \n",
    "WHERE quant = \"rad\" AND person = \"dyer\";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but this would be tedious, and if we ever had a data set with fifty or five hundred scientists, the chances of us getting all of those queries right is small.\n",
    "\n",
    "What we need to do is tell the database manager to aggregate the hours for each scientist separately using a GROUP BY clause:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT person, count(reading), round(avg(reading), 2) \n",
    "FROM Survey \n",
    "WHERE quant = \"rad\" \n",
    "GROUP BY person;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GROUP BY does exactly what its name implies: groups all the records with the same value for the specified field together so that aggregation can process each batch separately. Since all the records in each batch have the same value for person, it no longer matters that the database manager is picking an arbitrary one to display alongside the aggregated reading values.\n",
    "\n",
    "Just as we can sort by multiple criteria at once, we can also group by multiple criteria. To get the average reading by scientist and quantity measured, for example, we just add another field to the GROUP BY clause:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT person, quant, count(reading), round(avg(reading), 2) \n",
    "FROM Survey \n",
    "GROUP BY person, quant;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have added quant to the list of fields displayed, since the results wouldn’t make much sense otherwise.\n",
    "\n",
    "Let’s go one step further and remove all the entries where we don’t know who took the measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT person, quant, count(reading), round(avg(reading), 2) \n",
    "FROM Survey \n",
    "WHERE person IS NOT NULL \n",
    "GROUP BY person, quant \n",
    "ORDER BY person, quant;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more closely, this query:\n",
    "\n",
    "1. selected records from the Survey table where the person field was not null;\n",
    "2. grouped those records into subsets so that the person and quant values in each subset were the same;\n",
    "3. ordered those subsets first by person, and then within each sub-group by quant; and\n",
    "4. counted the number of records in each subset, calculated the average reading in each, and chose a person and quant value from each (it doesn’t matter which ones, since they’re all equal).\n",
    "\n",
    "How many temperature readings did Frank Pabodie record, and what was their average value?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average of a set of values is the sum of the values divided by the number of values. Does this mean that the avg function returns 2.0 or 3.0 when given the values 1.0, null, and 5.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the difference between each individual radiation reading and the average of all the radiation readings. We write the query:\n",
    "\n",
    "SELECT reading - avg(reading) FROM Survey WHERE quant = 'rad';\n",
    "\n",
    "What does this actually produce, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points:\n",
    "- Use aggregation functions to combine multiple values.\n",
    "- Aggregation functions ignore null values.\n",
    "- Aggregation happens after filtering.\n",
    "- Use GROUP BY to combine subsets separately.\n",
    "- If no aggregation function is specified for a field, the query may return an arbitrary value for that field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
